{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2g3dTpG5clsT"
      },
      "outputs": [],
      "source": [
        "!pip install sentence-transformers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_BaEJjERdVWf"
      },
      "outputs": [],
      "source": [
        "!pip install huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M7jglOoydifA"
      },
      "outputs": [],
      "source": [
        "!pip install langchain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tzn0VdcUwsqh"
      },
      "outputs": [],
      "source": [
        "!pip install faiss-gpu"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch"
      ],
      "metadata": {
        "id": "YMwe7UbqEGoc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "X2HW3uOvEL-U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install requests"
      ],
      "metadata": {
        "id": "qPMqWVSPEV5N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kM1b-27Ar4OH"
      },
      "outputs": [],
      "source": [
        "# GPU llama-cpp-python\n",
        "!CMAKE_ARGS=\"-DLLAMA_CUBLAS=on\" FORCE_CMAKE=1 pip install llama-cpp-python==0.1.78 --force-reinstall --upgrade --no-cache-dir --verbose"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip uninstall numpy -y"
      ],
      "metadata": {
        "id": "3zmzIVLqfyKm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install numpy==1.24.4"
      ],
      "metadata": {
        "id": "lwJG3HTIYLfk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install PyMuPDF"
      ],
      "metadata": {
        "id": "7A50-K6-whVD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Use if you have A100 runtime"
      ],
      "metadata": {
        "id": "372oS3cjpLr1"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "oI-kXwg5bHF-"
      },
      "outputs": [],
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q4_0.bin\" # the model is in bin format"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-13B-chat-GGML\"\n",
        "model_basename = \"llama-2-13b-chat.ggmlv3.q5_0.bin\" # the model is in bin format"
      ],
      "metadata": {
        "id": "z7wYlBSciWJx"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "model_basename = \"llama-2-7b-chat.ggmlv3.q8_0.bin\" # the model is in bin format"
      ],
      "metadata": {
        "id": "6_tt4A3ln6Vh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path = \"TheBloke/Llama-2-7B-Chat-GGML\"\n",
        "model_basename = \"llama-2-7b-chat.ggmlv3.q4_0.bin\" # the model is in bin format"
      ],
      "metadata": {
        "id": "1BMHywoxuV7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "QcPTl_QccnnL"
      },
      "outputs": [],
      "source": [
        "from langchain.document_loaders.csv_loader import CSVLoader\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.vectorstores.faiss import FAISS\n",
        "from huggingface_hub import hf_hub_download\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "from langchain.chains import ConversationalRetrievalChain\n",
        "from langchain.llms import CTransformers\n",
        "from langchain.llms import LlamaCpp\n",
        "import numpy as np\n",
        "import time\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import faiss\n",
        "import requests\n",
        "import random\n",
        "from llama_cpp import Llama\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import fitz\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_path = hf_hub_download(repo_id=model_name_or_path, filename=model_basename)"
      ],
      "metadata": {
        "id": "fhNX_mbkdc_K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "duK-Th6d1mtM"
      },
      "outputs": [],
      "source": [
        "torch.cuda.empty_cache()\n",
        "\n",
        "\n",
        "\n",
        "# Change the FAISS index path to use the local file\n",
        "DB_FAISS_PATH = \"/content/vectorstore/db_faiss/faiss.index\"\n",
        "\n",
        "# Download the model using HF-Hub\n",
        "\n",
        "lcpp_llm = None\n",
        "lcpp_llm = Llama(\n",
        "    model_path=model_path,\n",
        "    n_threads=2,  # CPU cores\n",
        "    n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "    n_gpu_layers=43,  # Change this value based on your model and your GPU VRAM pool.\n",
        "    n_ctx=4096,  # Context window\n",
        ")\n",
        "# lcpp_llm2 = Llama(\n",
        "#     model_path=model_path,\n",
        "#     n_threads=2,  # CPU cores\n",
        "#     n_batch=512,  # Should be between 1 and n_ctx, consider the amount of VRAM in your GPU.\n",
        "#     n_gpu_layers=43,  # Change this value based on your model and your GPU VRAM pool.\n",
        "#     n_ctx=4096,  # Context window\n",
        "# )\n",
        "\n",
        "# Load the dataset into a pandas DataFrame\n",
        "loader = CSVLoader(file_path=\"/content/data/mc-dataset.csv\", encoding=\"utf-8\", csv_args={'delimiter': ','})\n",
        "data = loader.load()\n",
        "\n",
        "# Change the text splitter import to use the langchain version\n",
        "text_splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=512, chunk_overlap=20\n",
        ")\n",
        "\n",
        "# Split the dataset into text chunks\n",
        "text_chunks = text_splitter.split_documents(data)\n",
        "text_strings = [doc.page_content for doc in text_chunks]\n",
        "\n",
        "# Initialize the model from the sentence-transformers library\n",
        "model = SentenceTransformer('sentence-transformers/paraphrase-xlm-r-multilingual-v1')\n",
        "\n",
        "\n",
        "def get_embeddings(texts):\n",
        "    try:\n",
        "        # texts should be a list of strings\n",
        "        embeddings = model.encode(texts, show_progress_bar=True)\n",
        "        print(\"Encoding completed successfully.\")\n",
        "        return embeddings\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during encoding: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "# Change the embedding import to use the langchain version\n",
        "embeddings = get_embeddings(text_strings)\n",
        "\n",
        "# Extract the embeddings from the FAISS index\n",
        "# docsearch = FAISS.from_documents(text_chunks, embeddings)\n",
        "\n",
        "# docsearch.save_local(DB_FAISS_PATH)\n",
        "dimension = embeddings.shape[1]  # Get the dimensionality of your embeddings\n",
        "index = faiss.IndexFlatL2(dimension)  # Create a flat (brute-force) search index\n",
        "\n",
        "# FAISS requires the data type to be float32\n",
        "if embeddings.dtype != np.float32:\n",
        "    embeddings = embeddings.astype(np.float32)\n",
        "\n",
        "index.add(embeddings)  # Add your embeddings to the index\n",
        "\n",
        "# To save the index to disk\n",
        "faiss.write_index(index, DB_FAISS_PATH)\n",
        "\n",
        "\n",
        "# Function to extract text from PDF files\n",
        "def extract_text_from_pdf(pdf_folder):\n",
        "    texts = []\n",
        "    for filename in os.listdir(pdf_folder):\n",
        "        if filename.endswith('.pdf'):\n",
        "            pdf_path = os.path.join(pdf_folder, filename)\n",
        "            with fitz.open(pdf_path) as pdf_doc:\n",
        "                text = \"\"\n",
        "                for page in pdf_doc:\n",
        "                    text += page.get_text()\n",
        "                texts.append(text)\n",
        "    return texts\n",
        "\n",
        "\n",
        "pdf_texts = extract_text_from_pdf(\"/content/data/quality-dataset/\")\n",
        "\n",
        "# Get embeddings for the PDF texts\n",
        "pdf_embeddings = get_embeddings(pdf_texts)\n",
        "\n",
        "# Initialize a new Faiss index\n",
        "pdf_index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "\n",
        "# Convert embeddings to float32 if necessary\n",
        "if pdf_embeddings.dtype != np.float32:\n",
        "    pdf_embeddings = pdf_embeddings.astype(np.float32)\n",
        "\n",
        "# Add PDF embeddings to the index\n",
        "pdf_index.add(pdf_embeddings)\n",
        "\n",
        "# Save the PDF Faiss index to disk\n",
        "faiss.write_index(pdf_index, \"pdf_faiss_index.index\")\n",
        "\n",
        "\n",
        "def load_faiss_index(index_path):\n",
        "    return faiss.read_index(index_path)\n",
        "\n",
        "\n",
        "def fetch_question_from_api(question_id):\n",
        "    \"\"\"Fetch a question from the API.\"\"\"\n",
        "    api_url = f'http://18.222.223.225:8000/questions/{question_id}'\n",
        "    try:\n",
        "        response = requests.get(api_url)\n",
        "        if response.status_code == 200:\n",
        "            return response.json()\n",
        "        else:\n",
        "            print(f\"Error fetching question: HTTP {response.status_code}\")\n",
        "            return None\n",
        "    except requests.RequestException as e:\n",
        "        print(f\"Exception when fetching question: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def generate_exam(num_questions):\n",
        "    \"\"\"Generate an exam with a specified number of questions.\"\"\"\n",
        "    exam = []\n",
        "    question_ids = random.sample(range(1, 490), num_questions)  # Randomly select question IDs\n",
        "    for q_id in question_ids:\n",
        "        question = fetch_question_from_api(q_id)\n",
        "        if question:\n",
        "            exam.append(question)\n",
        "        else:\n",
        "            print(f\"Failed to fetch question with ID: {q_id}\")\n",
        "    return exam  # Returns a list of question dictionaries\n",
        "\n",
        "\n",
        "def generate_response(prompt):\n",
        "    \"\"\"Generate a response using the specified agent.\"\"\"\n",
        "    response = lcpp_llm(\n",
        "        prompt=prompt,\n",
        "        max_tokens=1000,\n",
        "        temperature=0.5,\n",
        "        top_p=0.95,\n",
        "        repeat_penalty=1.2,\n",
        "        top_k=50,\n",
        "        stop=['USER:', 'ASSISTANT:'],\n",
        "        echo=True\n",
        "    )\n",
        "    return response[\"choices\"][0][\"text\"]\n",
        "\n",
        "\n",
        "def generate_response_pdf(prompt):\n",
        "    \"\"\"Generate a response using the specified agent.\"\"\"\n",
        "    max_tokens_per_segment = 4096  # Maximum number of tokens per segment\n",
        "    segments = [prompt[i:i+max_tokens_per_segment] for i in range(0, len(prompt), max_tokens_per_segment)]\n",
        "    responses = []\n",
        "    for segment in segments:\n",
        "        response = lcpp_llm(\n",
        "            prompt=segment,\n",
        "            max_tokens=1000,\n",
        "            temperature=0.5,\n",
        "            top_p=0.95,\n",
        "            repeat_penalty=1.2,\n",
        "            top_k=50,\n",
        "            stop=['USER:', 'ASSISTANT:'],\n",
        "            echo=True\n",
        "        )\n",
        "        responses.append(response[\"choices\"][0][\"text\"])\n",
        "    return ' '.join(responses)\n",
        "\n",
        "\n",
        "def agent_interaction(question, file_path, encoding_model, llm_model, index, question_id):\n",
        "    \"\"\"\n",
        "    Simulate the interaction based on a question object and store the chat history in the specified file.\n",
        "    If an index is provided, perform a similarity search to assist with answering the question.\n",
        "    \"\"\"\n",
        "    question_text = question['Question']\n",
        "    choices_text = f\" {question['Choice_A']}  {question['Choice_B']}  {question['Choice_C']}  {question['Choice_D']}  {question['Choice_E']}\"\n",
        "\n",
        "    prompt_for_agent = f\"Question: {question_text} Choices: {choices_text} + Only one answer is correct. Answer only with the letter of the correct answer. \\nAnswer:\"\n",
        "\n",
        "    # Perform similarity search if an index is provided\n",
        "    if index is not None:\n",
        "        question_embedding = encoding_model.encode([question_text], convert_to_tensor=True).cpu().numpy()\n",
        "        D, I = index.search(question_embedding, k=1)  # Adjust k based on how many similar items you want to consider\n",
        "\n",
        "        # Incorporate information from similar items into the prompt\n",
        "        # similar_item_info = \" Based on similar documents, consider focusing on aspects related to the question.\"\n",
        "        # prompt_for_agent += similar_item_info\n",
        "\n",
        "    response_from_agent = generate_response(prompt_for_agent)\n",
        "\n",
        "    # Save the interaction to the specified chat history file\n",
        "    save_chat_history(file_path, f\"Question ID: {question_id}\")\n",
        "    save_chat_history(file_path, f\"{response_from_agent}\")\n",
        "    save_chat_history(file_path, f\"Correct Answer: {question['Correct_Answer']}\")\n",
        "    save_chat_history(file_path, f\"-\" * 60)\n",
        "\n",
        "    print(f\"{response_from_agent}\")\n",
        "    print(f\"Correct Answer: {question['Correct_Answer']}\")\n",
        "\n",
        "    # print(f\"{prompt_for_agent}\")\n",
        "    # print(f\"{response_from_agent}\")\n",
        "    # print(f\"The correct answer is: {question['Correct_Answer']}\")\n",
        "\n",
        "    print(f\"{response_from_agent}\")\n",
        "    print(f\"Correct Answer: {question['Correct_Answer']}\")\n",
        "\n",
        "\n",
        "def save_chat_history(file_path, message):\n",
        "    \"\"\"Appends a message to the chat history file.\"\"\"\n",
        "    with open(file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        file.write(message + \"\\n\")\n",
        "\n",
        "\n",
        "def save_chat_history_pdf(file_path, messages):\n",
        "    \"\"\"Appends messages to the chat history file.\"\"\"\n",
        "    with open(file_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        for message in messages:\n",
        "            file.write(str(message) + \"\\n\")  # Ensure message is converted to string\n",
        "\n",
        "\n",
        "def main():\n",
        "    practice_chat_history_path = \"practice_chat_history.txt\"\n",
        "    exam_chat_history_path = \"exam_chat_history.txt\"\n",
        "    api_exam_results_path = \"api_exam_results.txt\"\n",
        "    pdf_exam_results_path = \"pdf_exam_results.txt\"\n",
        "    quality_chat_history_path = \"quality_conversation.txt\"\n",
        "\n",
        "    faiss_index = load_faiss_index(DB_FAISS_PATH)\n",
        "\n",
        "    # Practice phase for API questions\n",
        "    print(\"Starting practice phase for API questions...\")\n",
        "    api_practice_start_time = time.time()\n",
        "    for practice_question_id in range(1, 490):  # Assuming these are placeholder values\n",
        "        question = fetch_question_from_api(practice_question_id)\n",
        "        agent_interaction(question, practice_chat_history_path, model, lcpp_llm, None, practice_question_id)\n",
        "        print(f\"Completed practice question ID: {practice_question_id}\")\n",
        "        print(\"-\" * 30)\n",
        "    api_practice_end_time = time.time()\n",
        "    with open(practice_chat_history_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        file.write(f\"Total Practice Phase Time: {api_practice_end_time - api_practice_start_time} seconds\\n\")\n",
        "\n",
        "    # Generate the same exam for both groups\n",
        "    exam_questions = generate_exam(25)  # Adjust the number of questions as necessary\n",
        "\n",
        "    # Exam phase for API questions\n",
        "    print(\"Starting exam phase for API questions...\")\n",
        "    api_exam_start_time = time.time()\n",
        "    for question in exam_questions:\n",
        "        agent_interaction(question, exam_chat_history_path, model, lcpp_llm, faiss_index, question['ID'])\n",
        "    api_exam_end_time = time.time()\n",
        "    with open(api_exam_results_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        file.write(f\"Total Exam Time: {api_exam_end_time - api_exam_start_time} seconds\\n\")\n",
        "\n",
        "    # Practice phase for PDF data\n",
        "    print(\"Starting practice phase for PDF data...\")\n",
        "    pdf_practice_start_time = time.time()\n",
        "    for _ in range(100):\n",
        "        pdf_text = random.choice(pdf_texts)\n",
        "        agent1_prompt = f\"Discussion based on PDF content: {pdf_text[:100]}...\"  # Shorten for brevity\n",
        "        agent1_response = generate_response_pdf(agent1_prompt)\n",
        "        save_chat_history_pdf(quality_chat_history_path, [agent1_prompt, agent1_response, \"-\" * 60])\n",
        "    pdf_practice_end_time = time.time()\n",
        "    with open(quality_chat_history_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        file.write(f\"Total Practice Phase Time for PDF: {pdf_practice_end_time - pdf_practice_start_time} seconds\\n\")\n",
        "\n",
        "    # Exam phase for PDF data, using the same exam questions\n",
        "    print(\"Starting exam phase for PDF data...\")\n",
        "    pdf_exam_start_time = time.time()\n",
        "    for question in exam_questions:\n",
        "        agent_interaction(question, pdf_exam_results_path, model, lcpp_llm, pdf_index,\n",
        "                          question['ID'])  # Note the use of pdf_index if relevant\n",
        "    pdf_exam_end_time = time.time()\n",
        "    with open(pdf_exam_results_path, \"a\", encoding=\"utf-8\") as file:\n",
        "        file.write(f\"Total Exam Time for PDF: {pdf_exam_end_time - pdf_exam_start_time} seconds\\n\")\n",
        "\n",
        "    print(\"Practice and exam phases for both groups completed.\")\n",
        "\n",
        "    while True:\n",
        "        print(\"Sleep 5 min\")\n",
        "        time.sleep(300)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ]
}